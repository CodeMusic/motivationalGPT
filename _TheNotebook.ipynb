{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a11f9b-a710-4208-a46c-69b4416623ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class CognitiveClarifier(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfbd5074-ea37-4d71-b9e8-ebf8c839779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        \n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "808f8824-966b-4acf-86c7-964661549e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias) #c_fc or c_proj in the original MLP\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6623b7de-3ed1-410c-9b4f-cb8bd4fd9ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, numberOfNeurons, bias=True, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.neurons = nn.ModuleList()\n",
    "        for i in range(len(numberOfNeurons) - 1):\n",
    "            self.neurons.append(Neuron(numberOfNeurons[i], numberOfNeurons[i + 1], bias=bias, dropout=dropout))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for aNeuron in self.neurons:\n",
    "            x = aNeuron(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79e90d9b-976e-4f1d-86a7-65a92970b19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3800,  0.2443],\n",
      "        [-0.0059, -0.0246],\n",
      "        [ 0.0790, -0.1704],\n",
      "        [ 0.1027,  0.0710],\n",
      "        [ 0.0275, -0.0944],\n",
      "        [ 0.1436,  0.1779],\n",
      "        [ 0.2156, -0.1365],\n",
      "        [ 0.2178, -0.0000]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Tests:\n",
    "\n",
    "# Define a neural network with 3 layers: input layer with 5 neurons, hidden layer with 10 neurons, output layer with 2 neurons\n",
    "layers = [5, 10, 2]\n",
    "neural_network = NeuralNetwork(layers, bias=True, dropout=0.1)\n",
    "\n",
    "# Example tensor with shape (batch_size, input_features)\n",
    "x = torch.randn(8, 5)\n",
    "output = neural_network(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc052780-7e95-45ba-8267-7b4bd5014d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThoughtProcessor(nn.Module):\n",
    "    \"\"\"\n",
    "    A block consisting of multi-head self-attention, feed-forward network, layer normalization, and residual connections.\n",
    "    \n",
    "    Arguments:\n",
    "        config (object): Configuration object with attributes n_embd, n_head, dropout, block_size, bias.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = CognitiveClarifier(config.n_embd, bias=config.bias)\n",
    "        self.ln2 = CognitiveClarifier(config.n_embd, bias=config.bias)\n",
    "        self.attn = TemporalAttention(config)\n",
    "        self.mlp = NeuralNetwork([config.n_embd, 4 * config.n_embd, config.n_embd], bias=config.bias, dropout=config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Multi-head self-attention\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        # Feed-forward network\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18b3b4e6-8e69-4445-bc93-581c5eaaefeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128, 64])\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class NeuralCircuitSettings:\n",
    "    n_embd = 64\n",
    "    n_head = 8\n",
    "    dropout = 0.1\n",
    "    block_size = 128\n",
    "    bias = True\n",
    "    vocabSize = 50257 #vocabSize\n",
    "    n_layer = 12 #n_layer\n",
    "    bias = True\n",
    "\n",
    "config = NeuralCircuitSettings()\n",
    "block = ThoughtProcessor(config)\n",
    "\n",
    "# Example tensor with shape (batch_size, sequence_length, embedding_dim)\n",
    "x = torch.randn(32, 128, 64)\n",
    "output = block(x)\n",
    "print(output.size())  # Expected output size: (32, 128, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "411a96d5-6c43-4d52-81f1-f076e862bc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cortex(nn.Module):\n",
    "    \"\"\"\n",
    "    Cortex (GPT) is like the whole brain, composed of multiple ThoughtProcessors,\n",
    "    which together process input data sequentially to generate output, mimicking the flow of thought.\n",
    "    \"\"\"\n",
    "    def __init__(self, neuroConfig: NeuralCircuitSettings): \n",
    "        super().__init__()\n",
    "\n",
    "        if neuroConfig is None:\n",
    "            print(\"No configuration provided, using default settings.\")\n",
    "\n",
    "        assert neuroConfig.vocabSize is not None\n",
    "        assert neuroConfig.block_size is not None            \n",
    "\n",
    "        self.neuroConfig = neuroConfig\n",
    "\n",
    "        self.tokenEmbedding = nn.Embedding(neuroConfig.vocabSize, neuroConfig.n_embd)\n",
    "        self.positionalEmbedding = nn.Embedding(neuroConfig.block_size, neuroConfig.n_embd)\n",
    "        self.dropout = nn.Dropout(neuroConfig.dropout)\n",
    "        self.thoughtProcessors = nn.ModuleList([ThoughtProcessor(neuroConfig) for _ in range(neuroConfig.n_layer)])\n",
    "        self.outputNormalizer = CognitiveClarifier(neuroConfig.n_embd, bias=neuroConfig.bias)\n",
    "        self.outputLayer = nn.Linear(neuroConfig.n_embd, neuroConfig.vocabSize, bias=False)\n",
    "        self.tokenEmbedding.weight = self.outputLayer.weight  # Weight tying\n",
    "\n",
    "        self.apply(self._init_lessonScalingFactors)\n",
    "        \n",
    "        for pn, p in self.named_parameters():  # Apply special scaled init to the residual projections, per GPT-2 paper\n",
    "            if pn.endswith('outputProjection.weight') or pn.endswith('outputLayer.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * neuroConfig.n_layer))\n",
    "\n",
    "    def getNumberOfSynapses(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        synapsesCount_n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            synapsesCount_n_params -= self.positionalEmbedding.weight.numel()\n",
    "        return synapsesCount_n_params\n",
    "\n",
    "    def _init_lessonScalingFactors(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, dendrites_input, targets=None, external_context=None):\n",
    "        device = dendrites_input.device\n",
    "        cognitiveBatchSize, cognitiveSequenceLength = dendrites_input.size()\n",
    "        assert cognitiveSequenceLength <= self.neuroConfig.block_size, f\"Cannot forward sequence of length {cognitiveSequenceLength}, block size is only {self.neuroConfig.block_size}\"\n",
    "        temporalPositions = torch.arange(0, cognitiveSequenceLength, dtype=torch.long, device=device)\n",
    "\n",
    "        tokenEmbeddings = self.tokenEmbedding(dendrites_input)\n",
    "        positionalEmbeddings = self.positionalEmbedding(temporalPositions)\n",
    "        sensoryInput = self.dropout(tokenEmbeddings + positionalEmbeddings)\n",
    "        \n",
    "        for aThoughtProcessor in self.thoughtProcessors:\n",
    "            sensoryInput = aThoughtProcessor(sensoryInput)\n",
    "\n",
    "        behaviouralResponse = self.outputNormalizer(sensoryInput)\n",
    "        \n",
    "        if targets is not None:\n",
    "            logits = self.outputLayer(behaviouralResponse)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            logits = self.outputLayer(behaviouralResponse[:, [-1], :])\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def reduceThoughtProcessorSize_crop_block_size(self, block_size):\n",
    "        assert block_size <= self.neuroConfig.block_size\n",
    "        self.neuroConfig.block_size = block_size\n",
    "        self.positionalEmbedding.weight = nn.Parameter(self.positionalEmbedding.weight[:block_size])\n",
    "        for aThoughtProcessor in self.thoughtProcessors:\n",
    "            if hasattr(aThoughtProcessor.attn, 'bias'):\n",
    "                aThoughtProcessor.attn.bias = aThoughtProcessor.attn.bias[:,:,:block_size,:block_size]\n",
    "    \n",
    "    @classmethod\n",
    "    def loadMemories(cls, modelType, overrideArgs=None):\n",
    "        assert modelType in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        overrideArgs = overrideArgs or {}\n",
    "        \n",
    "        assert all(k == 'dropoutRate' for k in overrideArgs)\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(f\"Loading weights from pretrained GPT: {modelType}\")\n",
    "        \n",
    "        configArgs = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024),\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280),\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600),\n",
    "        }[modelType]\n",
    "        configArgs.update({'vocabSize': 50257, 'block_size': 1024, 'bias': True})\n",
    "    \n",
    "        if 'dropoutRate' in overrideArgs:\n",
    "            configArgs['dropout'] = overrideArgs['dropoutRate']\n",
    "    \n",
    "        neuroConfig = NeuralCircuitSettings(**configArgs)\n",
    "        model = cls(neuroConfig)\n",
    "        memories = model.state_dict()\n",
    "        memoryKeys_sd_keys = [key for key in memories.keys() if not key.endswith('.attn.bias')]\n",
    "    \n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(modelType)\n",
    "        memories_hf = model_hf.state_dict()\n",
    "        memories_keys_hf = [key for key in memories_hf.keys() if not key.endswith('attn.masked_bias') and not key.endswith('attn.bias')]\n",
    "    \n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        \n",
    "        assert len(memories_keys_hf) == len(memoryKeys_sd_keys), f\"Mismatched keys: {len(memories_keys_hf)} != {len(memoryKeys_sd_keys)}\"\n",
    "        for key in memories_keys_hf:\n",
    "            if any(key.endswith(w) for w in transposed):\n",
    "                assert memories_hf[key].shape[::-1] == memories[key].shape\n",
    "                with torch.no_grad():\n",
    "                    memories[key].copy_(memories_hf[key].t())\n",
    "            else:\n",
    "                assert memories_hf[key].shape == memories[key].shape\n",
    "                with torch.no_grad():\n",
    "                    memories[key].copy_(memories_hf[key])\n",
    "    \n",
    "        return model\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None, gradualOutput=False, external_context=None):\n",
    "        encode, decode = getCognitiveInterpreters()\n",
    "    \n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.neuroConfig.block_size else idx[:, -self.neuroConfig.block_size:]\n",
    "            logits, _ = self(idx_cond, external_context=external_context)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probabilities, num_samples=1)\n",
    "    \n",
    "            if idx_next.item() == 50256 or idx_next.item() == encode(\"\"):\n",
    "                print(\"\\nNatural Stop\\n\")\n",
    "                break\n",
    "    \n",
    "            if idx_next < 0:\n",
    "                raise ValueError(f\"Negative token index encountered: {idx_next}\")\n",
    "            \n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "            if gradualOutput:\n",
    "                next_token_text = decode([idx_next])\n",
    "                print(next_token_text, end='', flush=True)\n",
    "    \n",
    "        return decode(idx[0].tolist())\n",
    "    \n",
    "def getCognitiveInterpreters(metaPath=None):\n",
    "    if metaPath is None:\n",
    "        encoder = tiktoken.get_encoding(\"gpt2\")\n",
    "        encode = lambda s: encoder.encode(s, allowed_special={\"\"})\n",
    "        decode = lambda l: encoder.decode(l)\n",
    "    else:\n",
    "        if os.path.exists(metaPath):\n",
    "            print(f\"Loading meta from {metaPath}...\")\n",
    "            with open(metaPath, 'rb') as file:\n",
    "                meta = pickle.load(file)\n",
    "            toStringIndex, indexToString = meta['indexToString'], meta['stringToIndex']\n",
    "            encode = lambda s: [toStringIndex[c] for c in s]\n",
    "            decode = lambda l: ''.join([indexToString[I] for I in l])\n",
    "    return encode, decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48a1a27d-dd6e-4d67-a0bb-a164231aa79e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NeuralCircuitSettings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtiktoken\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mNeuralCircuitSettings\u001b[49m()\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m Cortex(config)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Example tensor with shape (batch_size, sequence_length)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NeuralCircuitSettings' is not defined"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "config = NeuralCircuitSettings()\n",
    "model = Cortex(config)\n",
    "# Example tensor with shape (batch_size, sequence_length)\n",
    "x = torch.randint(0, config.vocabSize, (2, 128))\n",
    "logits, loss = model(x)\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "if loss is not None:\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "# Test the generate method\n",
    "model.eval()\n",
    "generated_sequence = model.generate(x[:, :10], max_new_tokens=50, temperature=1.0, top_k=10, gradualOutput=True)\n",
    "print(f\"Generated sequence: {generated_sequence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3034ac23-1db1-4163-9ac0-a753a02cabe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
